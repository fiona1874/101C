---
title: "101CHW2"
author: "JING LI"
date: "4/13/2017"
output: pdf_document
---

1) (Make sure you have ggplot2 installed in Rstudio.  Ask me or the TA if you need help with this.You will then need to enter require(ggplot2).  You only need to do this once (but will have to do it again if you quit Rstudio and return and want to run ggplot2)).  Use ggplot2 to create a graphic, based on the LArealestate.csv data  (see Site Info/Data Not In Textbook), that shows us 3 (or more) variables on the same plot.   What questions does the graphic answer?

 
```{r}
library(readr)
library(ggplot2)
LArealestate <- read_csv("~/Downloads/LArealestate.csv")
head(LArealestate)
attach(LArealestate)
plot1 <- ggplot(LArealestate, aes(x=sqft, y=price, color = factor(beds)))+geom_point()
plot1
```
It appears that the real estate price is positively related to the area of the property, as well as the number of bedrooms and bathrooms.

2)  (a) Using the cdc.csv data (posted under Site Info/Data Not In Textbook) and ggplot2, make a plot that helps us understand the association between people's desired weight and their current weight, given their gender and whether or not they exercise. Your plot should include least squares lines to show the linear relation between desired weight and current weight for each of the four subgroups. Interpret these plots. (b) Instead of a regression line, use a smoother. Explain how the results differ from (a). (Note: You can  learn more about it at http://www.cdc.gov/brfss. I've posted the codebook in a separate file under Week 2. The data come from the Behavioral Risk Factor Survey System.

a)
```{r}
cdc <- read_csv("~/Downloads/cdc.csv")
p1 = ggplot(cdc, aes(x = wtdesire, y = weight, color = interaction(gender, exerany))) + geom_point()
p1
p2 = p1 + geom_smooth(method = "lm", se = FALSE, fullrange = TRUE)
p2
p1_alt = ggplot(cdc, aes(x = jitter(wtdesire), y = weight, color = interaction(gender, exerany))) + geom_point()
p1_alt
p3 = p1_alt + geom_smooth()
p3
p4_alt = p2 + facet_grid(gender~exerany)
p4_alt
```


3) In this question, you're going to make plots to show how the bias and variance change as the flexibility increases (see 2.12 for one example. These plots will be a bit more simplistic than 2.12, however.)

a) Write an R function that simulates observations from this model

```{r}
set.seed(123)
x = rep(0:10,5)
Y = 5 + 2*x + 1.5*x^2 + .4 *x^3+rnorm(length(x),0,10)
```


b) Fit a series of models: a linear model, a quadratic, a polynomial of order 3, order 4, order 5, order 6, and order 7.  For each, estimate the bias and the standard deviation at x_0 = 3.  Make a plot of bias against the order of the polynomial.  Connect the points with lines.

c) Superimpose, on the same plot, a plot of the variance of the model at x_0=3 against the order of the polynomial. (Again, connecting points with lines).

```{r}
truevalue <- 5 + 2*3 + 1.5*3^2 + 0.4*3^3

set.seed(123)
predicted1<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model1=lm(y~x)
  predicted1=c(predicted1, predict(model1,newdata=data.frame(x=3)))
}

set.seed(123)
predicted2<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model2=lm(y~x + I(x^2))
  predicted2=c(predicted2, predict(model2,newdata=data.frame(x=3)))
}

set.seed(123)
predicted3<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model3=lm(y~x + I(x^2) + I(x^3))
  predicted3=c(predicted3, predict(model3,newdata=data.frame(x=3)))
}

set.seed(123)
predicted4<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model4=lm(y~x + I(x^2) + I(x^3) + I(x^4))
  predicted4=c(predicted4, predict(model4,newdata=data.frame(x=3)))
}

set.seed(123)
predicted5<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model5=lm(y~x + I(x^2) + I(x^3) + I(x^4) + I(x^5))
  predicted5=c(predicted5, predict(model5,newdata=data.frame(x=3)))
}

set.seed(123)
predicted6<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model6=lm(y~x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
  predicted6=c(predicted6, predict(model6,newdata=data.frame(x=3)))
}

set.seed(123)
predicted7<-c()
for(i in 1:1000){
  y <- 5 + 2*x + 1.5*x^2 + 0.4*x^3 + rnorm(length(x),0,sqrt(10))
  model7=lm(y~x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7))
  predicted7=c(predicted7, predict(model7,newdata=data.frame(x=3)))
}

bias1 <- mean(predicted1) - truevalue
bias1
bias2 <- mean(predicted2) - truevalue
bias2 
bias3 <- mean(predicted3) - truevalue
bias3 
bias4 <- mean(predicted4) - truevalue
bias4 
bias5 <- mean(predicted5) - truevalue
bias5 
bias6 <- mean(predicted6) - truevalue
bias6 
bias7 <- mean(predicted7) - truevalue
bias7 

sd1 <- sd(predicted1)
sd1 
sd2 <- sd(predicted2)
sd2 
sd3 <- sd(predicted3)
sd3 
sd4 <- sd(predicted4)
sd4 
sd5 <- sd(predicted5)
sd5
sd6 <- sd(predicted6)
sd6 
sd7 <- sd(predicted7)
sd7 

var1<- var(predicted1)
var2<- var(predicted2)
var3<- var(predicted3)
var4<- var(predicted4)
var5<- var(predicted5)
var6<- var(predicted6)
var7<- var(predicted7)

bias <- c(bias1,bias2,bias3,bias4,bias5,bias6,bias7)
variance <- c(var1,var2,var3,var4,var5,var6,var7)
poly <- c(1:7)
bias.variance.poly <- data.frame(bias,variance,poly)
variance.poly <- data.frame(variance,poly)
ggplot(bias.variance.poly,aes(poly,bias,color="bias")) +geom_point() + geom_line() + geom_point(data = variance.poly,aes(poly,variance,color="variance")) + geom_line(data = variance.poly,aes(poly,variance,color="variance"))

```



d) Where does the bias have a minimum? Explain why this is not surprising. 

```{r}
order(abs(bias))
```
Bias6 is the smallest among all, which is not consistent with out true model. However, this is possible due to overfitting.

e) Calculate the MSE at x=3 and explain why the minimum is where it is at.

```{r}
MSE1<- var1+bias1^2
MSE2<- var2+bias2^2
MSE3<- var3+bias3^2
MSE4<- var4+bias4^2
MSE5<- var5+bias5^2
MSE6<- var6+bias6^2
MSE7<- var7+bias7^2
order(c(MSE1,MSE2,MSE3,MSE4,MSE5,MSE6,MSE7))
```

The MSE3 is the smallest. Indeed, our true model is a polynomial with degree of 3.

4) 2.4.7 (exclude part d)

a)
```{r}
sqrt(3^2) 
sqrt(2^2) 
sqrt(1+3^2) 
sqrt(1+2^2) 
sqrt(2) 
sqrt(3) Show in New WindowClear OutputExpand/Collapse Output
	
Removed 1 rows containing missing values (geom_point).

Modify Chunk OptionsRun All Chunks AboveRun Current Chunk
Show in New WindowClear OutputExpand/Collapse Output

Missing column names filled in: 'X1' [1]Parsed with column specification:
cols(
  X1 = col_integer(),
  state = col_integer(),
  genhlth = col_character(),
  physhlth = col_integer(),
  exerany = col_integer(),
  hlthplan = col_integer(),
  smoke100 = col_integer(),
  height = col_integer(),
  weight = col_integer(),
  wtdesire = col_integer(),
  age = col_integer(),
  gender = col_character(),
  hlth_notgood = col_logical()
)
R Console





Modify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current Chunk
Show in New WindowClear OutputExpand/Collapse Output
[1] 33.95731
[1] -11.04767
[1] -0.009803517
[1] -0.007834439
[1] -0.006050005
[1] 0.0002272833
[1] 0.0005674442
[1] 0.4886582
[1] 0.5656471
[1] 0.7637073
[1] 0.7663564
[1] 0.8760588
[1] 1.006668
[1] 1.008985
R Console


Show in New WindowClear OutputExpand/Collapse Output
[1] 6 7 5 4 3 2 1
Modify Chunk OptionsRun All Chunks AboveRun Current Chunk
Show in New WindowClear OutputExpand/Collapse Output
[1] 3 4 5 6 7 2 1
Console~/Downloads/
			
Console
R Markdown

	~/Downloads/101CHW2.Rmd		

			

```

b)
Since the closest observation is 5 and k=1, the prediction should be green.

c)
Since the closest observations are 5,6 and 2 adn k=3, the prediction here should be red.

